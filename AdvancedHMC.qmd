---
title: "AdvancedHMC.jl : A Tutorial"
format: 
  revealjs:
    scrollable: true
engine: julia
---

```{julia}
using Pkg
Pkg.instantiate()
```

Let's start by creating a target distribution to sample from. For this tutorial, we a multivariate normal target with identity covariance matrix

`AdvancedHMC` uses targets created by the `LogDensityProblems` package. This can be done in the code chunk below

```{julia}
using AdvancedHMC, ReverseDiff
using Turing, DynamicPPL, LogDensityProblems
using LinearAlgebra, SplittableRandoms, StatsPlots

struct LogTargetDensity{Tμ,TΣ}
    μ::Tμ
    Σ::TΣ
    dim::Int
end

LogDensityProblems.logdensity(p::LogTargetDensity, θ) = logpdf(MvNormal(p.μ,p.Σ),θ)
LogDensityProblems.dimension(p::LogTargetDensity) = p.dim

function LogDensityProblems.capabilities(::Type{LogTargetDensity})
    return LogDensityProblems.LogDensityOrder{0}()
end

# Choose parameter dimensionality and initial parameter value
rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
ℓπ = LogTargetDensity(μ, Σ, D)
```

We can also create the target using `Turing` models

```{julia}
@model function my_turing_model(μ,Σ)
  x ~ MvNormal(μ, Σ)
end

rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
model_instance = my_turing_model(μ,Σ)
ℓπ = LogDensityFunction(model_instance)
```

Now let's see `AdvancedHMC` in action with the popular No-U-Turn sampler (NUTS)

```{julia}
# specify metric
metric = DiagEuclideanMetric(D)

# specify hamiltonian
hamiltonian = Hamiltonian(metric, ℓπ, ReverseDiff)

# specify integrator
ϵ = 0.1
integrator = Leapfrog(ϵ)

# specify kernel
kernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))

# specify adaptor
δ = 0.8
adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(δ, integrator))


# initial chain values
rng = SplittableRandom(123)
n_samples, n_adapts = 2_000, 1_000
initial_θ = randn(rng,D)

# begin sampling
samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts)
```

To conveniently inspect the chain, we can use the following helper function to convert it in to a `Chains` object

```{julia}
function bundle_samples(
    samples, stats;
    param_names=missing,
    kwargs...,
)
    # state dimension
    d = length(samples[1])
    n = length(samples)

    # Turn all the transitions into a vector-of-vectors.
    vals = copy(reduce(hcat, [vcat(samples[i], vcat(values(stats[i])...)) for i in 1:n])')

    # Check if we received any parameter names.
    if ismissing(param_names)
        param_names = [Symbol("Parameter $i") for i in 1:d]
    end

    # Add the log density field to the parameter names.
    internal_params = vcat(keys(stats[1])...)
    push!(param_names, internal_params...)

    # Bundle everything up and return a Chains struct.
    return Chains(vals, param_names, (internals=internal_params,))
end
```

Finally, we can view and plot the result

```{julia}
chain = bundle_samples(samples, stats)

plot(chain)
```

# Detailed Usage

Before we continue, let's do a quick recap of Hamiltonian Monte Carlo (HMC). Consider a target distribution $\pi$ on $\mathbb{R}^d$. Most HMC methods sample from a joint target distribution in $\mathbb{R}^{2d}$

$$ H(q,p) = U(q) + K(p) = -\log\pi(q) + p^\top M^{-1}p $$

where $M$ is a $d\times d$ matrix called the *mass matrix* and $H$ is called the *Hamiltonian.* Intuitively, we can view $H,U$ and $K$ as the total, potential and kinetic of a physical system*.* At each iteration, the sampler, at a high level, does the following:

1.  **Proposal**: from the current point $z = (q,p)$,

    -   Move $z$ similar to rolling **a ball on the landscape** $U(q)$ with **initial velocity (momentum)** $p$ for a period of time $t$

    -   Choose a point $z'$ along the trajectory of the ball and use it as the proposal

    For a visual demonstration of different HMC trajectories, see [trajectory simulator](https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&target=banana).

2.  Perform an **MH accept/reject step** based on that proposal.

3.  Sample a new momentum.

Now, we can parse the code chunk at the beginning

-   `metric`: Specifies the mass matrix $M$.

-   `hamiltonian`: Specifies $H$. Requires

    -   a metric
    -   a kinetic (default to Gaussian kinetic i.e. $p^\top M^{-1}p$)
    -   a `LogDensityProblems` target
    -   a way to compute the gradient of the target (by an explicit function or AD package)

-   `integrator` specifies how the ball is moved along its path. Note that

    -   Moving the ball continuously is impossible.

    -   Instead, a discrete approximation of the ball's path is used.

-   `kernel` specifies trajectory length and how to propose a point from a trajectory.

    -   Trajectory length can be fixed beforehand or decided dynamically using a stopping criterion (e.g. No-U-Turn criterion).

    -   Some common proposals include choosing the last point in the trajectory or randomly choosing a point from the trajectory weighted by their Hamiltonian energy.

-   `adaptor`: specifies how the mass matrix and integrator step size are adapted.

In our example above, we used

-   a diagonal mass matrix

-   a fixed step size leapfrog integrator and the NUTS trajectory kernel (No-U-Turn criterion + Multinomial sampling from trajectories).

-   an adaptor that changes the mass matrix to match parameter sample variance and step size to achieve a certain acceptance rate for the proposals.

There are of course more options to choose from in the `AdvancedHMC`. For more information, see [AdvancedHMC docs](https://turinglang.org/AdvancedHMC.jl/dev/api/).

As an exercise, let's define a different HMC sampler and run it on our target

```{julia}
#| echo: false

# specify metric
metric = 

# specify hamiltonian
hamiltonian = 

# specify integrator
integrator = 

# specify kernel
kernel = 

# specify adaptor
adaptor = 

# initial chain values
rng = SplittableRandom(123)
n_samples, n_adapts = 2_000, 1_000
initial_θ = randn(rng,D)

# begin sampling
samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts)

# chain output
chain = bundle_samples(samples, stats)
```

## More customization

For more flexible customization, we can also modify the functions in the package corresponding to the aspects of interest. Here are some examples of what functions to modify to accommodate certain aspects:

-   For a new integrator, create its corresponding `struct` and modify the `step` function.

-   For a new metric, create its corresponding `struct` and modify `rand_momentum`, `neg_energy`, `∂H∂r`.