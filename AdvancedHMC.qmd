---
title: "AdvancedHMC.jl : A Tutorial"
format: 
  revealjs:
    scrollable: true
engine: julia
---

fsdfasdfsdfas

```{julia}
using Pkg
Pkg.instantiate()
```

Let's start by creating a target distribution to sample from. For this tutorial, we a multivariate normal target with identity covariance matrix

`AdvancedHMC` uses targets created by the `LogDensityProblems` package. This can be done in the code chunk below

```{julia}
using AdvancedHMC, ReverseDiff
using Turing, DynamicPPL, LogDensityProblems
using LinearAlgebra, SplittableRandoms, StatsPlots

struct LogTargetDensity{Tμ,TΣ}
    μ::Tμ
    Σ::TΣ
    dim::Int
end

LogDensityProblems.logdensity(p::LogTargetDensity, θ) = logpdf(MvNormal(p.μ,p.Σ),θ)
LogDensityProblems.dimension(p::LogTargetDensity) = p.dim

function LogDensityProblems.capabilities(::Type{LogTargetDensity})
    return LogDensityProblems.LogDensityOrder{0}()
end

# Choose parameter dimensionality and initial parameter value
rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
ℓπ = LogTargetDensity(μ, Σ, D)
```

We can also create the target using `Turing` models

```{julia}
@model function my_turing_model(μ,Σ)
  x ~ MvNormal(μ, Σ)
end

rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
model_instance = my_turing_model(μ,Σ)
ℓπ = LogDensityFunction(model_instance)
```

Next, we specify our MCMC sampler. The ingredients for this are:

-   **Metric:** This allows us to measure distances between points. In most samplers, this is represent by a $D\times D$ matrix called the mass matrix.

-   **Integrator:** This allows us to integrate along trajectories of the Hamiltonian dynamics via discretization. The leapfrog integrator is the most widely used integrator for many HMC samplers due it having relatively well-behaved numerical error with respect to arbitrary discretization step size.

-   **Trajectory kernel:** This allows us to choose points from trajectories generated by the integrator to use as proposals for the next point in the chain. There are two design aspects to trajectory kernels: trajectory length and a method to select points from trajectories. Trajectory length can be fixed beforehand or decided dynamically using a stopping criterion (e.g. No-U-Turn criterion). Once a trajectory has been generated, we can select points from it as proposals. Some common choices include choosing the last point in the trajectory or randomly choosing a point from the trajectory weighted by their Hamiltonian energy.

To start sampling, we need to create the following objects:

-   `hamiltonian`: This encompasses the information about the target Hamiltonian. It requires a metric, a kinetic, a `LogDensityProblems` target and a way to compute the gradient of the target (by an explicit function or AD package). By default, the kinetic is Gaussian.

-   `HMCkernel`: This combines two steps: momentum refreshment followed by trajectory sampling. By default, the momentum is fully refreshed but we can also partially refresh the momentum (i.e. the momentum of the next sample can depend on the momentum of the last sample).

-   `adaptor`: This encompasses the adaptation procedures for the mass matrix and integrator step size.

In our example, we use a diagonal mass matrix, a fixed step size leapfrog integrator and the NUTS trajectory kernel (No-U-Turn criterion + Multinomial sampling from trajectories). For the adaptor, we adapt the mass matrix to match parameter sample variance and step size to achieve a certain acceptance rate for the proposals. There are of course more options to choose from in the `AdvancedHMC`. For more information, see https://turinglang.org/AdvancedHMC.jl/dev/api/

```{julia}
# specify metric
metric = DiagEuclideanMetric(D)

# specify hamiltonian
hamiltonian = Hamiltonian(metric, ℓπ, ReverseDiff)

# specify integrator
ϵ = 0.1
integrator = Leapfrog(ϵ)

# specify kernel
kernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))

# specify adaptor
δ = 0.8
adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(δ, integrator))
```

Finally, we put all these elements along with the initial sample into the `sample` function as follows

```{julia}
rng = SplittableRandom(123)
n_samples, n_adapts = 2_000, 1_000
initial_θ = randn(rng,D)

samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts)
```

To conveniently inspect the chain, we can use the following helper function to convert it in to a `Chains` object

```{julia}
function bundle_samples(
    samples, stats;
    param_names=missing,
    kwargs...,
)
    # state dimension
    d = length(samples[1])
    n = length(samples)

    # Turn all the transitions into a vector-of-vectors.
    vals = copy(reduce(hcat, [vcat(samples[i], vcat(values(stats[i])...)) for i in 1:n])')

    # Check if we received any parameter names.
    if ismissing(param_names)
        param_names = [Symbol("Parameter $i") for i in 1:d]
    end

    # Add the log density field to the parameter names.
    internal_params = vcat(keys(stats[1])...)
    push!(param_names, internal_params...)

    # Bundle everything up and return a Chains struct.
    return Chains(vals, param_names, (internals=internal_params,))
end
```

Finally, we can view and plot the result

```{julia}
chain = bundle_samples(samples, stats)

plot(chain)
```

For more flexible customization, we can also modify the functions in the package corresponding to the aspects of interest. Here are some examples of what functions to modify to accommodate certain aspects:

-   For a new integrator, create its corresponding `struct` and modify the `step` function.

-   For a new metric, create its corresponding `struct` and modify `rand_momentum`, `neg_energy`, `∂H∂r`.