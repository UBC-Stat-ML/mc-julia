---
title: "AdvancedHMC.jl : A Tutorial"
engine: julia
---

```{julia}
#| echo: false
#| output: false

using Pkg
Pkg.instantiate()
```

Let's start by creating a target distribution to sample from. For this tutorial, we a multivariate normal target with identity covariance matrix

`AdvancedHMC` uses targets created by the `LogDensityProblems` package. This can be done in the code chunk below

```{julia}
using AdvancedHMC, ReverseDiff
using LogDensityProblems, MCMCChains
using LinearAlgebra, SplittableRandoms, StatsPlots
using Distributions

struct LogTargetDensity{Tμ,TΣ}
    μ::Tμ
    Σ::TΣ
    dim::Int
end

LogDensityProblems.logdensity(p::LogTargetDensity, θ) = logpdf(MvNormal(p.μ,p.Σ),θ)
LogDensityProblems.dimension(p::LogTargetDensity) = p.dim

function LogDensityProblems.capabilities(::LogTargetDensity)
    return LogDensityProblems.LogDensityOrder{0}()
end

# Choose parameter dimensionality and initial parameter value
rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
ℓπ = LogTargetDensity(μ, Σ, D);
```

Now let's see `AdvancedHMC` in action with the popular No-U-Turn sampler (NUTS)

```{julia}
#| output: false

# specify metric
metric = DiagEuclideanMetric(D)

# specify hamiltonian
hamiltonian = Hamiltonian(metric, ℓπ, ReverseDiff)

# specify integrator
ϵ = 0.1
integrator = Leapfrog(ϵ)

# specify kernel
kernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))

# specify adaptor
δ = 0.8
adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(δ, integrator))


# initial chain values
rng = SplittableRandom(123)
n_samples, n_adapts = 2_000, 1_000
initial_θ = randn(rng,D)

# begin sampling
samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts);
```

To conveniently inspect the chain, we can use the following helper function to convert it in to a `Chains` object

```{julia}
function bundle_samples(
    samples, stats=nothing;
    param_names=missing,
    kwargs...,
)
    # state dimension
    d = length(samples[1])
    n = length(samples)

    # Turn all the transitions into a vector-of-vectors.
    if isnothing(stats)
        vals = copy(reduce(hcat, [samples[i] for i in 1:n])')
    else
        vals = copy(reduce(hcat, [vcat(samples[i], vcat(values(stats[i])...)) for i in 1:n])')
    end

    # Check if we received any parameter names.
    if ismissing(param_names)
        param_names = [Symbol("Parameter $i") for i in 1:d]
    end

    if isnothing(stats)
        # Bundle everything up and return a Chains struct.
        return Chains(vals, param_names)
    else
        # Add internal parameter names.
        internal_params = vcat(keys(stats[1])...)
        push!(param_names, internal_params...)

        # Bundle everything up and return a Chains struct.
        return Chains(vals, param_names, (internals=internal_params,))
    end    
end
```

Finally, we can view and plot the result

```{julia}
chain = bundle_samples(samples, stats)
```

```{julia}
plot(chain)
```

# Detailed Usage

Before we continue, let's do a quick recap of Hamiltonian Monte Carlo (HMC). Consider a target distribution $\pi$ on $\mathbb{R}^d$. Most HMC methods sample from a joint target distribution in $\mathbb{R}^{2d}$

$$ H(q,p) = U(q) + K(p) = -\log\pi(q) + p^\top M^{-1}p $$

where $M$ is a $d\times d$ matrix called the *mass matrix* and $H$ is called the *Hamiltonian.* Intuitively, we can view $H,U$ and $K$ as the total, potential and kinetic of a physical system*.* At each iteration, the sampler, at a high level, does the following:

1.  **Proposal**: from the current point $z = (q,p)$,

    -   Move $z$ similar to rolling **a ball on the landscape** $U(q)$ with **initial velocity (momentum)** $p$ for a period of time $t$

    -   Choose a point $z'$ along the trajectory of the ball and use it as the proposal

    For a visual demonstration of different HMC trajectories, see [trajectory simulator](https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&target=banana).

2.  Perform an **MH accept/reject step** based on that proposal.

3.  Sample a new momentum.

Now, we can parse the code chunk at the beginning

-   `metric`: Specifies the mass matrix $M$.

-   `hamiltonian`: Specifies $H$. Requires

    -   a metric
    -   a kinetic (default to Gaussian kinetic i.e. $p^\top M^{-1}p$)
    -   a `LogDensityProblems` target
    -   a way to compute the gradient of the target (by an explicit function or AD package)

-   `integrator` specifies how the ball is moved along its path. Note that

    -   Moving the ball continuously is impossible.

    -   Instead, a discrete approximation of the ball's path is used.

-   `kernel` specifies trajectory length and how to propose a point from a trajectory.

    -   Trajectory length can be fixed beforehand or decided dynamically using a stopping criterion (e.g. No-U-Turn criterion).

    -   Some common proposals include choosing the last point in the trajectory or randomly choosing a point from the trajectory weighted by their Hamiltonian energy.

-   `adaptor`: specifies how the mass matrix and integrator step size are adapted.

In our example above, we used

-   a diagonal mass matrix

-   a fixed step size leapfrog integrator and the NUTS trajectory kernel (No-U-Turn criterion + Multinomial sampling from trajectories).

-   an adaptor that changes the mass matrix to match parameter sample variance and step size to achieve a certain acceptance rate for the proposals.

There are of course more options to choose from in the `AdvancedHMC`. For more information, see [AdvancedHMC docs](https://turinglang.org/AdvancedHMC.jl/dev/api/).

As an exercise, let's define a vanilla HMC sampler with 20 leapfrog steps and run it on our target

::: {.callout-caution collapse="true"}
## Click for answer
```{julia}
#| output: false

# specify metric
metric = DiagEuclideanMetric(D)

# specify hamiltonian
hamiltonian = Hamiltonian(metric, ℓπ, ReverseDiff)

# specify integrator
ϵ = 0.1
integrator = Leapfrog(ϵ)

# specify kernel
n_leapfrog = 20
kernel = HMCKernel(Trajectory{EndPointTS}(integrator, FixedNSteps(n_leapfrog)))

# specify adaptor
δ = 0.8
adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(δ, integrator))

# initial chain values
rng = SplittableRandom(123)
n_samples, n_adapts = 2_000, 1_000
initial_θ = randn(rng,D)

# begin sampling
samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts)
```

Now let's look at and visualize the output
```{julia}
# chain output
chain_HMC = bundle_samples(samples, stats)
```
```{julia}
plot(chain_HMC)
```
:::

# More customization

For more flexible customization, we can also modify the functions in the package corresponding to the aspects of interest. Here are some examples of what functions to modify to accommodate certain aspects:

-   For a new integrator, create its corresponding `struct` and modify the `step` function.

-   For a new metric, create its corresponding `struct` and modify `rand_momentum`, `neg_energy`, `∂H∂r`.

# More on `LogDensityProblems`

As seen in the beginning, to specify a target in `LogDensityProblems`, we need the following

-   a `struct` for the target

-   the target log density function

-   dimension of the target

-   capabilities of the target, i.e. the degree of derivative to specify

We can also create the target using `Turing` models

```{julia}
using Turing

@model function my_turing_model(μ,Σ)
  x ~ MvNormal(μ, Σ)
end

rng = SplittableRandom(123)
D = 10;
μ = randn(rng,D);
Σ = I
model_instance = my_turing_model(μ,Σ)
ℓπ = LogDensityFunction(model_instance);
```