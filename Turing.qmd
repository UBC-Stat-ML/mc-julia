---
title: "Turing.jl : A Tutorial"
format: 
  revealjs:
    scrollable: true
engine: julia
---

```{julia}
using Pkg
Pkg.instantiate()
```

Let's start by creating a model using `Turing`. To do this, we use the `@model` macro. In this macro, we specify distribution statement using the `~` notation. For example, if variable `x` in our model has $N(0,1)$, we can simply write it as `x~Normal(0,1)`. Aside from this the rest of the macro syntax follows regular julia code including transformations, assignments, indexing, etc.
As an example let's write the following Bayesian Logistic Regression model in `Turing`
$$
\begin{align}
a_i &\sim N(0,\sigma), &i=1,\ldots,4 \\
y_n &\sim \text{Bern}\left(\text{logistic}\left(\sum_{i=1}^4a_ix_{ni}\right)\right) & n=1,\ldots,N
\end{align}
$$
```{julia}
using Turing, DynamicPPL, LogDensityProblems
using LinearAlgebra, SplittableRandoms, StatsPlots
using Distributions, LogExpFunctions
@model function bayes_logistic(x, y, N, σ)
    a = Vector{Float64}(undef, 4)
    for i in eachindex(a)
        a[i] ~ Normal(0, σ) 
    end

    for n in 1:N
        v = logistic(dot(a,x[n,:]))
        y[n] ~ Bernoulli(v)
    end
end
```

With this model, we can simulate a dataset and create a corresponding posterior distribution object

```{julia}
# simulate dataset
rng = SplittableRandom(123)
N = 30
a = [10, 3.5, -0.7, -4.1]
x = randn(rng, 30, 4)
y = [rand(rng,Bernoulli(logistic(dot(a,x[n,:])))) for n in 1:N]

# create posterior distribution object
σ = 1.0
model = bayes_logistic(x, y, N, σ)
```

Now we can sample from `model` using the `sample` function. Let's using the No-U-Turn sampler (NUTS) as our MCMC sampler here

```{julia}
# begin sampling
chain = sample(model, NUTS(), 1000, progress=false)
```

We can also plot out the chain trace plots and density estimations

```{julia}
# plot results
plot(chain)
```

